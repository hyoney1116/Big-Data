{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Hani Soni**\n",
        "\n",
        "## **202318044**"
      ],
      "metadata": {
        "id": "2Yr97NxHjyYv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task C: Construct a dataset by scraping a website online using BeautifulSoup."
      ],
      "metadata": {
        "id": "t_aUqC-Rj7pH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xCpV-TfiwQE"
      },
      "outputs": [],
      "source": [
        "from selenium import webdriver\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium.webdriver.common.keys import Keys\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import time\n",
        "from rich import print\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def get_page_source(url):\n",
        "    pages = []\n",
        "    options = Options()\n",
        "    # options.add_argument(\"--headless=true\")\n",
        "    driver = webdriver.Chrome(options=options)\n",
        "    driver.maximize_window()\n",
        "    driver.get(url)\n",
        "    time.sleep(10)\n",
        "    search_box = driver.find_element(\n",
        "        \"xpath\", \"//input[@id='twotabsearchtextbox']\")\n",
        "    search_box.send_keys(\"Samsung Mobile\")\n",
        "    search_box.send_keys(Keys.ENTER)\n",
        "    time.sleep(2)\n",
        "    page_source = driver.page_source\n",
        "    pages.append(page_source)\n",
        "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "    i = 1\n",
        "    products = []\n",
        "    products.extend(scrape_data(page_source))\n",
        "    while i < 20:\n",
        "        next_button = soup.find(\"a\", attrs={\n",
        "                                \"class\": \"s-pagination-item s-pagination-next s-pagination-button s-pagination-separator\"})\n",
        "        if next_button:\n",
        "            i += 1\n",
        "            time.sleep(2)  # Add a delay to allow the next page to fully load\n",
        "            try:\n",
        "                driver.find_element(\n",
        "                    \"xpath\", \"//a[@class='s-pagination-item s-pagination-next s-pagination-button s-pagination-separator']\").click()\n",
        "                time.sleep(5)  # Add a delay after clicking the next button\n",
        "                page_source = driver.page_source\n",
        "                # Update soup with new page content\n",
        "                soup = BeautifulSoup(page_source, \"html.parser\")\n",
        "                p = scrape_data(page_source)\n",
        "                if p:\n",
        "                    products.extend(p)\n",
        "            except Exception as e:\n",
        "                print(f\"Error: {e}\")\n",
        "                break\n",
        "    driver.quit()\n",
        "    print(len(products))\n",
        "    return products\n",
        "\n",
        "\n",
        "def scrape_data(page):\n",
        "    products = []\n",
        "\n",
        "    soup = BeautifulSoup(page, \"html.parser\")\n",
        "    cards = soup.find_all(\n",
        "        \"div\", attrs={\"data-component-type\": \"s-search-result\"})\n",
        "    BASE_URL = \"https://www.amazon.in\"\n",
        "    for card in cards:\n",
        "        product = {}\n",
        "        image_tag = card.find(\n",
        "            \"img\", attrs={\"data-image-latency\": \"s-product-image\"}) or \"\"\n",
        "        img_link = image_tag and image_tag[\"src\"] or \"\"\n",
        "        title = card.find(\"a\", attrs={\n",
        "            \"class\": \"a-link-normal s-underline-text s-underline-link-text s-link-style a-text-normal\"}) or \"\"\n",
        "        url = title and BASE_URL + title[\"href\"]\n",
        "        title = title and title.get_text()\n",
        "        price = card.find(\n",
        "            'span', class_='a-price').find('span', class_='a-offscreen') or \"\"\n",
        "        price = price and price.get_text()\n",
        "        delivery_info = card.find(\n",
        "            \"div\", attrs={\"data-cy\": \"delivery-recipe\"}) or \"\"\n",
        "        delivery_info = delivery_info and delivery_info.get_text().strip()\n",
        "        print(card.get_text())\n",
        "\n",
        "        product[\"title\"] = title\n",
        "        product[\"img_url\"] = img_link\n",
        "        product[\"url\"] = url\n",
        "        product[\"price\"] = price\n",
        "        product[\"delivery_info\"] = delivery_info\n",
        "        products.append(product)\n",
        "\n",
        "    return products\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    url = \"https://www.amazon.in/\"\n",
        "    products = get_page_source(url)\n",
        "    df = pd.DataFrame(products)\n",
        "    df.to_excel(\"amazon_products.xlsx\")\n"
      ]
    }
  ]
}